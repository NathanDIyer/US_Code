#!/usr/bin/env python3
"""
High-Performance Ollama Integration for USC Search Dashboard

Optimized for speed with:
- qwen2.5:0.5b model (3-5x faster than gemma3:1b)
- Response caching for instant repeated queries
- Optimized prompts for legal text analysis
- Performance timing and monitoring
- Model preloading to eliminate cold starts

Compatible with PythonAnywhere (no threading, subprocess-based)
"""

import subprocess
import json
import hashlib
import time
import os
from typing import Optional, Dict, Any
from functools import lru_cache
import pickle
from datetime import datetime, timedelta

class OptimizedOllamaIntegration:
    def __init__(self, model_name: str = "qwen2.5:0.5b"):
        self.model_name = model_name
        self.cache_dir = "ai_cache"
        self.cache_expiry_hours = 0  # Caching disabled - always generate fresh responses
        self.performance_stats = {
            'total_requests': 0,
            'cache_hits': 0,
            'avg_response_time': 0,
            'fastest_response': float('inf'),
            'slowest_response': 0
        }
        
        # Create cache directory
        if not os.path.exists(self.cache_dir):
            os.makedirs(self.cache_dir)
        
        self.available = self._check_ollama_availability()
        
        if self.available:
            print(f"âœ“ Optimized Ollama integration ready with {model_name}")
            # Preload the model for faster first response
            self._preload_model()
        else:
            print(f"âš ï¸  Ollama not available - AI features disabled")
    
    def _check_ollama_availability(self) -> bool:
        """Check if Ollama is available and the model is installed"""
        try:
            result = subprocess.run(
                ["ollama", "list"], 
                capture_output=True, 
                text=True, 
                timeout=10
            )
            
            if result.returncode == 0:
                if self.model_name in result.stdout:
                    return True
                else:
                    print(f"Model {self.model_name} not found. Available models:")
                    print(result.stdout)
                    # Try fallback to gemma3:1b if qwen2.5:0.5b not found
                    if "gemma3:1b" in result.stdout and self.model_name == "qwen2.5:0.5b":
                        print(f"Falling back to gemma3:1b")
                        self.model_name = "gemma3:1b"
                        return True
                    return False
            return False
            
        except Exception as e:
            print(f"Ollama check failed: {e}")
            return False
    
    def _preload_model(self):
        """Preload the model to eliminate cold start delays"""
        try:
            print("ðŸ”¥ Preloading AI model for faster responses...")
            start_time = time.time()
            
            result = subprocess.run(
                ["ollama", "run", self.model_name, "Ready."],
                capture_output=True,
                text=True,
                timeout=15
            )
            
            preload_time = time.time() - start_time
            if result.returncode == 0:
                print(f"âœ“ Model preloaded in {preload_time:.1f}s - subsequent requests will be faster")
            else:
                print(f"âš ï¸  Model preload failed: {result.stderr}")
                
        except Exception as e:
            print(f"Model preload failed: {e}")
    
    def _get_cache_key(self, content: str, question: str) -> str:
        """Generate a cache key for the question and content"""
        # Create a hash of the content and question for caching
        content_hash = hashlib.md5(content.encode()).hexdigest()[:8]
        question_hash = hashlib.md5(question.lower().strip().encode()).hexdigest()[:8]
        return f"{content_hash}_{question_hash}"
    
    def _get_cached_response(self, cache_key: str) -> Optional[str]:
        """Caching disabled - always return None to force fresh responses"""
        return None
    
    def _cache_response(self, cache_key: str, response: str):
        """Caching disabled - no responses stored"""
        pass
    
    def _call_ollama_optimized(self, prompt: str, max_tokens: int = 300) -> Optional[str]:
        """Optimized Ollama call with performance monitoring"""
        if not self.available:
            return None
        
        start_time = time.time()
        
        try:
            # Optimize prompt length for the smaller model
            if len(prompt) > 4000:  # Smaller limit for 0.5B model
                prompt = prompt[:4000] + "..."
            
            result = subprocess.run(
                ["ollama", "run", self.model_name, prompt],
                capture_output=True,
                text=True,
                timeout=20  # Shorter timeout for faster model
            )
            
            response_time = time.time() - start_time
            
            # Update performance stats
            self.performance_stats['total_requests'] += 1
            self.performance_stats['avg_response_time'] = (
                (self.performance_stats['avg_response_time'] * (self.performance_stats['total_requests'] - 1) + response_time) 
                / self.performance_stats['total_requests']
            )
            self.performance_stats['fastest_response'] = min(self.performance_stats['fastest_response'], response_time)
            self.performance_stats['slowest_response'] = max(self.performance_stats['slowest_response'], response_time)
            
            if result.returncode == 0 and result.stdout.strip():
                return result.stdout.strip()
            else:
                print(f"Ollama error: {result.stderr}")
                return None
                
        except subprocess.TimeoutExpired:
            print("Ollama request timed out")
            return None
        except Exception as e:
            print(f"Error calling Ollama: {e}")
            return None
    
    def ask_about_section_optimized(self, section_content: str, question: str) -> Dict[str, Any]:
        """Ask a question about a USC section with performance tracking (no caching)"""
        start_time = time.time()
        
        if not self.available:
            return {
                'response': "AI assistant not available. Please check that Ollama is running with qwen2.5:0.5b model.",
                'cached': False,
                'response_time': 0,
                'model': self.model_name
            }
        
        if not section_content.strip() or not question.strip():
            return {
                'response': "Please provide both section content and a question.",
                'cached': False,
                'response_time': 0,
                'model': self.model_name
            }
        
        # Create optimized prompt for legal analysis
        prompt = f"""Legal Analysis Assistant

Section: {section_content[:3000]}

Question: {question}

Provide a clear, helpful answer about this US Code section. Be concise and practical."""
        
        response = self._call_ollama_optimized(prompt)
        
        if response:
            return {
                'response': response,
                'cached': False,
                'response_time': time.time() - start_time,
                'model': self.model_name
            }
        else:
            return {
                'response': "Sorry, I couldn't generate a response. Please try again.",
                'cached': False,
                'response_time': time.time() - start_time,
                'model': self.model_name
            }
    
    def ask_about_section(self, section_content: str, question: str) -> str:
        """Legacy method for backwards compatibility"""
        result = self.ask_about_section_optimized(section_content, question)
        return result['response']
    
    def get_performance_stats(self) -> Dict[str, Any]:
        """Get current performance statistics"""
        stats = self.performance_stats.copy()
        
        if stats['total_requests'] > 0:
            cache_hit_rate = (stats['cache_hits'] / stats['total_requests']) * 100
            stats['cache_hit_rate'] = f"{cache_hit_rate:.1f}%"
            stats['avg_response_time'] = f"{stats['avg_response_time']:.2f}s"
            stats['fastest_response'] = f"{stats['fastest_response']:.2f}s" if stats['fastest_response'] != float('inf') else "N/A"
            stats['slowest_response'] = f"{stats['slowest_response']:.2f}s"
        else:
            stats['cache_hit_rate'] = "0%"
            stats['avg_response_time'] = "N/A"
            stats['fastest_response'] = "N/A"
            stats['slowest_response'] = "N/A"
        
        return stats
    
    def clear_cache(self):
        """Clear all cached responses"""
        try:
            for file in os.listdir(self.cache_dir):
                if file.endswith('.pkl'):
                    os.remove(os.path.join(self.cache_dir, file))
            print("âœ“ AI response cache cleared")
        except Exception as e:
            print(f"Error clearing cache: {e}")
    
    def is_available(self) -> bool:
        """Check if AI features are available"""
        return self.available

# Global instance
_ollama = None

def get_ollama() -> OptimizedOllamaIntegration:
    """Get the global optimized Ollama integration instance"""
    global _ollama
    if _ollama is None:
        _ollama = OptimizedOllamaIntegration()
    return _ollama

def ask_ai(section_content: str, question: str) -> str:
    """Simple function to ask AI about a section"""
    ollama = get_ollama()
    return ollama.ask_about_section(section_content, question)

def ask_ai_with_stats(section_content: str, question: str) -> Dict[str, Any]:
    """Ask AI with performance statistics"""
    ollama = get_ollama()
    return ollama.ask_about_section_optimized(section_content, question)

def get_performance_stats() -> Dict[str, Any]:
    """Get AI performance statistics"""
    ollama = get_ollama()
    return ollama.get_performance_stats()

def clear_ai_cache():
    """Clear AI response cache"""
    ollama = get_ollama()
    ollama.clear_cache()

def is_ai_available() -> bool:
    """Check if AI is available"""
    ollama = get_ollama()
    return ollama.is_available() 